{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVHpDVfUIF5q2bj13E6Smq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narenpradhan/Pulse-Diagnosis-System/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn xgboost sdv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SRXv9YtTwL4b",
        "outputId": "3bdc3373-b36a-41e6-d078-76c936f7dc6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Collecting sdv\n",
            "  Downloading sdv-1.29.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Collecting boto3<2.0.0,>=1.28 (from sdv)\n",
            "  Downloading boto3-1.40.74-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting botocore<2.0.0,>=1.31 (from sdv)\n",
            "  Downloading botocore-1.40.74-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (3.1.2)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from sdv) (0.21)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.12/dist-packages (from sdv) (4.67.1)\n",
            "Collecting copulas>=0.12.1 (from sdv)\n",
            "  Downloading copulas-0.12.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting ctgan>=0.11.0 (from sdv)\n",
            "  Downloading ctgan-0.11.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting deepecho>=0.7.0 (from sdv)\n",
            "  Downloading deepecho-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting rdt>=1.18.2 (from sdv)\n",
            "  Downloading rdt-1.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sdmetrics>=0.21.0 (from sdv)\n",
            "  Downloading sdmetrics-0.24.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.12/dist-packages (from sdv) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from sdv) (6.0.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3<2.0.0,>=1.28->sdv)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.5.0)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.12/dist-packages (from copulas>=0.12.1->sdv) (5.24.1)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from ctgan>=0.11.0->sdv) (2.8.0+cu126)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting Faker!=37.11.0,>=17 (from rdt>=1.18.2->sdv)\n",
            "  Downloading faker-38.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly>=5.10.0->copulas>=0.12.1->sdv) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->ctgan>=0.11.0->sdv) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->ctgan>=0.11.0->sdv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->ctgan>=0.11.0->sdv) (3.0.3)\n",
            "Downloading sdv-1.29.0-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.6/196.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.40.74-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.74-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading copulas-0.12.3-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctgan-0.11.1-py3-none-any.whl (25 kB)\n",
            "Downloading deepecho-0.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading rdt-1.18.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sdmetrics-0.24.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m198.3/198.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-38.0.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, Faker, botocore, s3transfer, rdt, copulas, sdmetrics, deepecho, ctgan, boto3, sdv\n",
            "Successfully installed Faker-38.0.0 boto3-1.40.74 botocore-1.40.74 copulas-0.12.3 ctgan-0.11.1 deepecho-0.7.0 jmespath-1.0.1 rdt-1.18.2 s3transfer-0.14.0 sdmetrics-0.24.0 sdv-1.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox-ylB42wMsI",
        "outputId": "3e0f8943-58e6-4d02-efcd-312370ef4cd2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyP37Xl8vNUL",
        "outputId": "abaedfa8-6f9b-4579-e8e6-6060cee7591b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 'Updated_Prakriti_With_Features.csv' (1200 rows)\n",
            "Successfully loaded 'synthetic_prakriti_data_5000.csv' (5000 rows)\n",
            "\n",
            "--- Starting Baseline Model (1200 Real Rows) ---\n",
            "Processed features shape: (1200, 87)\n",
            "Training features: (960, 87), Testing features: (240, 87)\n",
            "Training model...\n",
            "Training complete.\n",
            "\n",
            "--- Baseline Model (1200 Real Rows) Results ---\n",
            "Overall Model Accuracy: 100.00%\n",
            "\n",
            "Successfully combined data. New shape: (6200, 30)\n",
            "\n",
            "--- Starting Combined Model (6200 Rows) ---\n",
            "Processed features shape: (6200, 87)\n",
            "Training features: (4960, 87), Testing features: (1240, 87)\n",
            "Training model...\n",
            "Training complete.\n",
            "\n",
            "--- Combined Model (6200 Rows) Results ---\n",
            "Overall Model Accuracy: 61.37%\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Kapha       0.50      0.54      0.52        70\n",
            "       Pitta       0.48      0.27      0.34       161\n",
            "        Vata       0.59      0.58      0.59       238\n",
            " pitta+kapha       0.34      0.18      0.24        78\n",
            "  vata+kapha       0.48      0.17      0.25        65\n",
            "  vata+pitta       0.67      0.82      0.74       628\n",
            "\n",
            "    accuracy                           0.61      1240\n",
            "   macro avg       0.51      0.43      0.45      1240\n",
            "weighted avg       0.59      0.61      0.59      1240\n",
            "\n",
            "\n",
            "--- üèÜ Final Comparison ---\n",
            "Baseline Accuracy (1200 rows): 100.00%\n",
            "New Accuracy (6200 rows):   61.37%\n"
          ]
        }
      ],
      "source": [
        "## V1 - OLD NOT ACCURATE - ROTE LEARNING\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for a cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Function to load, preprocess, train, and evaluate ---\n",
        "def train_and_evaluate(data, le, description):\n",
        "    \"\"\"\n",
        "    A helper function to run the full pipeline on a given dataset.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Starting {description} ---\")\n",
        "\n",
        "    # 1. Separate Features (X) and Target (y)\n",
        "    X = data.drop(columns=['Dosha'])\n",
        "    y = data['Dosha']\n",
        "\n",
        "    # 2. Preprocess Features (One-Hot Encoding)\n",
        "    # We fit a new encoder for each dataset (baseline vs. combined)\n",
        "    # as the combined one might have slightly different categories.\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    X_encoded = ohe.fit_transform(X)\n",
        "\n",
        "    # 3. Preprocess Target (Label Encoding)\n",
        "    # We use the *same* LabelEncoder 'le' fit on the real data\n",
        "    # to ensure class numbers (e.g., 'Pitta' -> 1) are consistent.\n",
        "    y_encoded = le.transform(y)\n",
        "\n",
        "    print(f\"Processed features shape: {X_encoded.shape}\")\n",
        "\n",
        "    # 4. Split Data (80% train, 20% test)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_encoded, y_encoded,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_encoded # Ensures balanced classes in train/test\n",
        "    )\n",
        "    print(f\"Training features: {X_train.shape}, Testing features: {X_test.shape}\")\n",
        "\n",
        "    # 5. Initialize and Train XGBoost Model\n",
        "    model = XGBClassifier(\n",
        "        objective='multi:softmax',\n",
        "        num_class=len(le.classes_), # Auto-detect number of classes\n",
        "        random_state=42,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss'\n",
        "    )\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    # 6. Evaluate Model\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\n--- {description} Results ---\")\n",
        "    print(f\"Overall Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # For the final combined model, print the full report\n",
        "    if \"Combined\" in description:\n",
        "        print(\"\\nDetailed Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# --- Main Execution ---\n",
        "try:\n",
        "    # Load real data\n",
        "    # We use dtype=str to prevent the same TypeError as before\n",
        "    real_data = pd.read_csv('/content/drive/MyDrive/Final Year Project/Updated_Prakriti_With_Features.csv', dtype=str)\n",
        "    print(f\"Successfully loaded 'Updated_Prakriti_With_Features.csv' ({len(real_data)} rows)\")\n",
        "\n",
        "    # Load synthetic data\n",
        "    synthetic_data = pd.read_csv('/content/drive/MyDrive/Final Year Project/synthetic_prakriti_data_5000.csv', dtype=str)\n",
        "    print(f\"Successfully loaded 'synthetic_prakriti_data_5000.csv' ({len(synthetic_data)} rows)\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading file: {e}. Please ensure both CSV files are present.\")\n",
        "    exit()\n",
        "\n",
        "# --- Baseline Model (Real Data Only) ---\n",
        "# First, create and fit the LabelEncoder on the REAL data's target\n",
        "le = LabelEncoder()\n",
        "le.fit(real_data['Dosha'])\n",
        "\n",
        "# Now, run the baseline test\n",
        "baseline_accuracy = train_and_evaluate(real_data, le, \"Baseline Model (1200 Real Rows)\")\n",
        "\n",
        "# --- Combined Model (Real + Synthetic Data) ---\n",
        "# 1. Combine the datasets\n",
        "combined_df = pd.concat([real_data, synthetic_data], ignore_index=True)\n",
        "print(f\"\\nSuccessfully combined data. New shape: {combined_df.shape}\")\n",
        "\n",
        "# 2. Run the full pipeline on the new combined data\n",
        "new_accuracy = train_and_evaluate(combined_df, le, \"Combined Model (6200 Rows)\")\n",
        "\n",
        "# --- Final Comparison ---\n",
        "print(\"\\n--- üèÜ Final Comparison ---\")\n",
        "print(f\"Baseline Accuracy (1200 rows): {baseline_accuracy * 100:.2f}%\")\n",
        "print(f\"New Accuracy (6200 rows):   {new_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## V2\n",
        "import pandas as pd\n",
        "from ctgan import CTGAN\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "import time\n",
        "import joblib\n",
        "import os # <-- New import to check if file exists\n",
        "\n",
        "# Suppress all warnings for a clean output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"--- Starting Corrected ML Pipeline ---\")\n",
        "\n",
        "# --- Step 1: Load and Split Real Data ---\n",
        "try:\n",
        "    real_data = pd.read_csv('/content/drive/MyDrive/Final Year Project/Updated_Prakriti_With_Features.csv', dtype=str)\n",
        "    print(f\"\\nSuccessfully loaded real data ({len(real_data)} rows).\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nError: 'Updated_Prakriti_With_Features.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "# The \"Golden Rule\": Split REAL data first.\n",
        "# We will train the synthesizer on 'real_train_df' and test our final model on 'real_test_df'\n",
        "real_train_df, real_test_df = train_test_split(\n",
        "    real_data,\n",
        "    test_size=0.2,  # 20% held back for final, real testing\n",
        "    random_state=42,\n",
        "    stratify=real_data['Dosha'] # Ensure classes are balanced in split\n",
        ")\n",
        "print(f\"Real data split: {len(real_train_df)} for training, {len(real_test_df)} for final testing.\")\n",
        "\n",
        "\n",
        "# --- Step 2: Generate or Load Synthetic Data ---\n",
        "synthetic_data_filename = '/content/drive/MyDrive/Final Year Project/synthetic_prakriti_data_5000.csv'\n",
        "num_new_rows = 5000\n",
        "\n",
        "# --- This is the new logic ---\n",
        "if os.path.exists(synthetic_data_filename):\n",
        "    print(f\"\\nFound existing synthetic data: '{synthetic_data_filename}'. Loading file...\")\n",
        "    synthetic_data = pd.read_csv(synthetic_data_filename, dtype=str)\n",
        "    print(f\"Successfully loaded {len(synthetic_data)} synthetic rows.\")\n",
        "else:\n",
        "    print(f\"\\nNo existing synthetic data found. Generating new data...\")\n",
        "    print(\"Initializing CTGAN synthesizer...\")\n",
        "    synthesizer = CTGAN(\n",
        "        epochs=300,  # You can increase this to 500 or 1000 for better quality\n",
        "        batch_size=50,\n",
        "        verbose=False # Set to True if you want to see training progress\n",
        "    )\n",
        "\n",
        "    print(\"Training synthesizer on real training data (this may take several minutes)...\")\n",
        "    start_time = time.time()\n",
        "    synthesizer.fit(real_train_df)\n",
        "    print(f\"Synthesizer training complete in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    print(f\"Generating {num_new_rows} new synthetic samples...\")\n",
        "    synthetic_data = synthesizer.sample(num_rows=num_new_rows)\n",
        "\n",
        "    print(f\"Saving new synthetic data to '{synthetic_data_filename}' for future use...\")\n",
        "    synthetic_data.to_csv(synthetic_data_filename, index=False)\n",
        "# --- End of new logic ---\n",
        "\n",
        "\n",
        "# --- Step 3: Combine Real Training + Synthetic Data ---\n",
        "combined_train_df = pd.concat([real_train_df, synthetic_data], ignore_index=True)\n",
        "print(f\"Created new combined training set with {len(combined_train_df)} rows.\")\n",
        "\n",
        "\n",
        "# --- Step 4: Preprocessing (Encoding) ---\n",
        "print(\"\\nPreprocessing data (One-Hot & Label Encoding)...\")\n",
        "target_column = 'Dosha'\n",
        "\n",
        "# 1. Label Encode the Target ('Dosha')\n",
        "le = LabelEncoder()\n",
        "# Fit *only* on the real training data\n",
        "le.fit(real_train_df[target_column])\n",
        "\n",
        "# Transform all our target sets\n",
        "y_train_combined = le.transform(combined_train_df[target_column])\n",
        "y_test_final = le.transform(real_test_df[target_column])\n",
        "\n",
        "# 2. One-Hot Encode the Features\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "# Get feature columns (all columns except 'Dosha')\n",
        "feature_cols = [col for col in real_data.columns if col != target_column]\n",
        "\n",
        "# Fit *only* on the combined training data\n",
        "ohe.fit(combined_train_df[feature_cols])\n",
        "\n",
        "# Transform all our feature sets\n",
        "X_train_combined = ohe.transform(combined_train_df[feature_cols])\n",
        "X_test_final = ohe.transform(real_test_df[feature_cols])\n",
        "\n",
        "print(f\"New training features shape: {X_train_combined.shape}\")\n",
        "print(f\"Final test features shape: {X_test_final.shape}\")\n",
        "\n",
        "\n",
        "# --- Step 5: Train the *Regularized* XGBoost Model ---\n",
        "print(\"\\nInitializing *REGULARIZED* XGBoost model...\")\n",
        "# These parameters are \"handcuffs\" to prevent overfitting (memorizing)\n",
        "model = XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=len(le.classes_),\n",
        "    random_state=42,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric='mlogloss',\n",
        "\n",
        "    # --- Regularization Parameters ---\n",
        "    n_estimators=300,       # More trees (to compensate for low learning rate)\n",
        "    learning_rate=0.05,     # Slows down learning\n",
        "    max_depth=4,            # Shorter trees (less complex)\n",
        "    subsample=0.8,          # Use 80% of data for each tree\n",
        "    colsample_bytree=0.8    # Use 80% of features for each tree\n",
        ")\n",
        "\n",
        "print(\"Training new model on combined data...\")\n",
        "start_time = time.time()\n",
        "model.fit(X_train_combined, y_train_combined)\n",
        "print(f\"Model training complete in {time.time() - start_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "# --- Step 6: Test on the \"Golden\" *Real* Test Set ---\n",
        "print(f\"\\n--- üèÜ FINAL RESULTS (Tested on {len(real_test_df)} REAL rows) ---\")\n",
        "\n",
        "y_pred_final = model.predict(X_test_final)\n",
        "final_accuracy = accuracy_score(y_test_final, y_pred_final)\n",
        "\n",
        "print(f\"\\nFinal Model Accuracy: {final_accuracy * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nFinal Classification Report:\")\n",
        "print(classification_report(y_test_final, y_pred_final, target_names=le.classes_))\n",
        "\n",
        "\n",
        "# --- Step 7: Save Model + Encoders ---\n",
        "save_dir = \"/content/drive/MyDrive/Final Year Project/prakriti_model_assets\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "joblib.dump(model, f\"{save_dir}/xgb_prakriti_model.pkl\")\n",
        "joblib.dump(le, f\"{save_dir}/label_encoder.pkl\")\n",
        "joblib.dump(ohe, f\"{save_dir}/onehot_encoder.pkl\")\n",
        "\n",
        "print(\"\\nModels saved successfully:\")\n",
        "print(f\"- XGBoost model        ‚Üí {save_dir}/xgb_prakriti_model.pkl\")\n",
        "print(f\"- Label Encoder        ‚Üí {save_dir}/label_encoder.pkl\")\n",
        "print(f\"- OneHot Encoder       ‚Üí {save_dir}/onehot_encoder.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcTQ6aJYy-s-",
        "outputId": "f9150a91-ab80-46b9-88d7-62dd8eb47a75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Corrected ML Pipeline ---\n",
            "\n",
            "Successfully loaded real data (1200 rows).\n",
            "Real data split: 960 for training, 240 for final testing.\n",
            "\n",
            "Found existing synthetic data: '/content/drive/MyDrive/Final Year Project/synthetic_prakriti_data_5000.csv'. Loading file...\n",
            "Successfully loaded 5000 synthetic rows.\n",
            "Created new combined training set with 5960 rows.\n",
            "\n",
            "Preprocessing data (One-Hot & Label Encoding)...\n",
            "New training features shape: (5960, 87)\n",
            "Final test features shape: (240, 87)\n",
            "\n",
            "Initializing *REGULARIZED* XGBoost model...\n",
            "Training new model on combined data...\n",
            "Model training complete in 2.09 seconds.\n",
            "\n",
            "--- üèÜ FINAL RESULTS (Tested on 240 REAL rows) ---\n",
            "\n",
            "Final Model Accuracy: 97.92%\n",
            "\n",
            "Final Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Kapha       1.00      1.00      1.00        14\n",
            "       Pitta       1.00      0.97      0.98        29\n",
            "        Vata       1.00      0.98      0.99        53\n",
            " pitta+kapha       1.00      0.90      0.95        10\n",
            "  vata+kapha       1.00      0.78      0.88         9\n",
            "  vata+pitta       0.96      1.00      0.98       125\n",
            "\n",
            "    accuracy                           0.98       240\n",
            "   macro avg       0.99      0.94      0.96       240\n",
            "weighted avg       0.98      0.98      0.98       240\n",
            "\n",
            "\n",
            "Models saved successfully:\n",
            "- XGBoost model        ‚Üí /content/drive/MyDrive/Final Year Project/prakriti_model_assets/xgb_prakriti_model.pkl\n",
            "- Label Encoder        ‚Üí /content/drive/MyDrive/Final Year Project/prakriti_model_assets/label_encoder.pkl\n",
            "- OneHot Encoder       ‚Üí /content/drive/MyDrive/Final Year Project/prakriti_model_assets/onehot_encoder.pkl\n"
          ]
        }
      ]
    }
  ]
}